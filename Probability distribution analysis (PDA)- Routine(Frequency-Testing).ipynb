{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3a8f70",
   "metadata": {},
   "source": [
    "# Probability distribution analysis - Routine(Frequency-Testing)\n",
    "\n",
    "This notebook provides a **clean, reproducible routine** for **frequency analysis** and **probability distribution testing** using annual hydroclimate time series derived from gridded NetCDF data (e.g., runoff, precipitation).\n",
    "\n",
    "**Typical outputs (for Supplementary Information):**\n",
    "- Annual aggregation (calendar year or water year)\n",
    "- Distribution fitting (**Normal**, **Gumbel (EV1)**, **GEV**, **Log-Pearson Type III**)\n",
    "- Goodness-of-fit (GOF) table (KS statistic/p-value + AIC/BIC)\n",
    "- Return levels for selected return periods (e.g., 2–100 years)\n",
    "- Optional: pixel-wise return level maps + NetCDF export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd6cbe",
   "metadata": {},
   "source": [
    "## Data source note (CMIP6)\n",
    "\n",
    "**CMIP6 hydroclimate projections used in this workflow can be obtained from ORNL IMPACT:**\n",
    "https://impact.ornl.gov/en/datasets/cmip6-based-multi-model-hydroclimate-projection-over-the-contermi/\n",
    "\n",
    "(Kept here so readers can trace dataset provenance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3a879",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "1. Set paths in **Configuration** (NetCDF file(s), variable name hint, region shapefile).\n",
    "2. Run **Example A** for a 1D (region-mean) frequency analysis.\n",
    "3. (Optional) Run **Example B** for a pixel-wise return level map.\n",
    "\n",
    "Notes:\n",
    "- For frequency analysis, **annual maxima** (block maxima) is common, but this notebook also supports **annual totals**.\n",
    "- Log-Pearson III requires **strictly positive** samples (values ≤ 0 are dropped for that fit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (uncomment if needed in a fresh environment):\n",
    "# !pip install -q numpy pandas xarray netCDF4 h5netcdf cftime scipy matplotlib geopandas rioxarray shapely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm, gumbel_r, genextreme as gev, pearson3\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Optional geospatial dependencies (needed only for clipping/mapping)\n",
    "import geopandas as gpd\n",
    "import rioxarray  # activates the `.rio` accessor\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07383c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "# CMIP6 hydroclimate projection data source (ORNL IMPACT):\n",
    "# https://impact.ornl.gov/en/datasets/cmip6-based-multi-model-hydroclimate-projection-over-the-contermi/\n",
    "\n",
    "# --- Input NetCDF(s) ---\n",
    "# Provide a single file OR a list of files. If you have separate historical/future files, set both.\n",
    "RUNOFF_HIST_FILE = r\"/path/to/runoff_monthly_1980_2019.nc\"\n",
    "RUNOFF_FUT_FILE  = r\"/path/to/runoff_monthly_2020_2059.nc\"   # optional\n",
    "\n",
    "# If you only have one file with the full period, put it in RUNOFF_HIST_FILE and set RUNOFF_FUT_FILE=None\n",
    "# RUNOFF_FUT_FILE = None\n",
    "\n",
    "# --- Variable name ---\n",
    "# The code will try to auto-detect, but this hint helps (common names: \"runoff\", \"mrro\", \"RAIN\", \"qtot\")\n",
    "VAR_HINT = \"runoff\"\n",
    "\n",
    "# --- Region of interest ---\n",
    "REGION_SHP = r\"/path/to/region_boundary.shp\"  # e.g., Iowa state boundary\n",
    "REGION_ID_FIELD = None  # set to a field name if you want to select one polygon; None = use all geometries\n",
    "\n",
    "# --- Annual aggregation ---\n",
    "USE_WATER_YEAR = False  # False = calendar year (Jan-Dec); True = water year (Oct-Sep)\n",
    "\n",
    "# Choose one:\n",
    "ANNUAL_STATISTIC = \"total\"  # \"total\" or \"max\"\n",
    "# \"total\": sum monthly values within each year\n",
    "# \"max\"  : maximum monthly value within each year (block maxima)\n",
    "\n",
    "# --- Periods (used for slicing the annual series) ---\n",
    "HIST_Y0, HIST_Y1 = 1980, 2022\n",
    "FUT_Y0,  FUT_Y1  = 2023, 2059\n",
    "\n",
    "# --- Return periods (years) ---\n",
    "RETURN_PERIODS = [2, 5, 10, 25, 50, 100]\n",
    "\n",
    "# --- Output directory ---\n",
    "OUT_DIR = r\"/path/to/output_folder\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "\n",
    "def _find_data_var(ds: xr.Dataset, var_hint: str | None = None) -> str:\n",
    "    # Pick a reasonable data variable from a Dataset\n",
    "    if var_hint and var_hint in ds.data_vars:\n",
    "        return var_hint\n",
    "    for candidate in [\"runoff\", \"mrro\", \"RAIN\", \"qtot\", \"Q\", \"discharge\"]:\n",
    "        if candidate in ds.data_vars:\n",
    "            return candidate\n",
    "    if len(ds.data_vars) == 1:\n",
    "        return list(ds.data_vars)[0]\n",
    "    raise ValueError(f\"Could not determine variable. Available: {list(ds.data_vars)}\")\n",
    "\n",
    "def _open_netcdf(paths):\n",
    "    # Open one or many NetCDF files robustly\n",
    "    if paths is None:\n",
    "        return None\n",
    "    if isinstance(paths, (list, tuple)):\n",
    "        ds = xr.open_mfdataset(list(paths), combine=\"by_coords\", parallel=True, use_cftime=True)\n",
    "    else:\n",
    "        ds = xr.open_dataset(paths, use_cftime=True)\n",
    "    return ds\n",
    "\n",
    "def _ensure_latlon(da: xr.DataArray) -> xr.DataArray:\n",
    "    # Ensure the DataArray is recognized as having spatial dimensions that rioxarray can work with\n",
    "    dims = list(da.dims)\n",
    "    lat_dim = next((d for d in dims if d.lower() in (\"lat\",\"latitude\",\"y\")), None)\n",
    "    lon_dim = next((d for d in dims if d.lower() in (\"lon\",\"longitude\",\"x\")), None)\n",
    "    if lat_dim is None or lon_dim is None:\n",
    "        raise ValueError(f\"Could not infer lat/lon dims from {dims}\")\n",
    "    da = da.rio.set_spatial_dims(x_dim=lon_dim, y_dim=lat_dim, inplace=False)\n",
    "    if da.rio.crs is None:\n",
    "        da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    return da\n",
    "\n",
    "def clip_to_region(da: xr.DataArray, shp_path: str, id_field: str | None = None, id_value=None) -> xr.DataArray:\n",
    "    # Clip a gridded DataArray to a polygon region (shapefile)\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    if id_field is not None and id_value is not None:\n",
    "        gdf = gdf[gdf[id_field] == id_value]\n",
    "    if gdf.empty:\n",
    "        raise ValueError(\"Region shapefile selection returned no geometries.\")\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    da = _ensure_latlon(da)\n",
    "    return da.rio.clip(gdf.geometry.apply(mapping), gdf.crs, drop=True)\n",
    "\n",
    "def annual_aggregate(da_monthly: xr.DataArray, statistic: str = \"total\", water_year: bool = False) -> xr.DataArray:\n",
    "    # Convert monthly data to annual series (total or max)\n",
    "    if \"time\" not in da_monthly.dims:\n",
    "        raise ValueError(\"Expected a 'time' dimension.\")\n",
    "    t = da_monthly[\"time\"]\n",
    "\n",
    "    if water_year:\n",
    "        year = xr.where(t.dt.month >= 10, t.dt.year + 1, t.dt.year)  # label by ending year\n",
    "    else:\n",
    "        year = t.dt.year\n",
    "\n",
    "    if statistic == \"total\":\n",
    "        ann = da_monthly.groupby(year).sum(\"time\", skipna=True)\n",
    "    elif statistic == \"max\":\n",
    "        ann = da_monthly.groupby(year).max(\"time\", skipna=True)\n",
    "    else:\n",
    "        raise ValueError(\"statistic must be 'total' or 'max'\")\n",
    "\n",
    "    ann = ann.rename({\"group\": \"year\"} if \"group\" in ann.dims else {})\n",
    "    ann.attrs.update(da_monthly.attrs)\n",
    "    ann.attrs[\"annual_statistic\"] = statistic\n",
    "    ann.attrs[\"water_year\"] = str(bool(water_year))\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86475481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Distribution fitting + return levels\n",
    "# =========================\n",
    "\n",
    "def _clean_sample(x):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    return x\n",
    "\n",
    "def fit_normal(x):\n",
    "    x = _clean_sample(x)\n",
    "    mu, sigma = norm.fit(x)\n",
    "    return (mu, sigma)\n",
    "\n",
    "def fit_gumbel(x):\n",
    "    x = _clean_sample(x)\n",
    "    loc, scale = gumbel_r.fit(x)\n",
    "    return (loc, scale)\n",
    "\n",
    "def fit_gev(x):\n",
    "    x = _clean_sample(x)\n",
    "    c, loc, scale = gev.fit(x)\n",
    "    return (c, loc, scale)\n",
    "\n",
    "def fit_lp3(x):\n",
    "    # Log-Pearson Type III fit on base-10 log(x); requires x>0\n",
    "    x = _clean_sample(x)\n",
    "    x = x[x > 0]\n",
    "    if x.size < 10:\n",
    "        raise ValueError(\"LP3 needs at least 10 positive samples.\")\n",
    "    y = np.log10(x)\n",
    "    skew, loc, scale = pearson3.fit(y)\n",
    "    return (skew, loc, scale)\n",
    "\n",
    "def return_level(dist_name: str, params: tuple, T: float):\n",
    "    # Return level for return period T using non-exceedance probability p = 1 - 1/T\n",
    "    p = 1.0 - 1.0/float(T)\n",
    "\n",
    "    if dist_name == \"Normal\":\n",
    "        mu, sigma = params\n",
    "        return float(norm.ppf(p, loc=mu, scale=sigma))\n",
    "\n",
    "    if dist_name == \"Gumbel\":\n",
    "        loc, scale = params\n",
    "        return float(gumbel_r.ppf(p, loc=loc, scale=scale))\n",
    "\n",
    "    if dist_name == \"GEV\":\n",
    "        c, loc, scale = params\n",
    "        return float(gev.ppf(p, c, loc=loc, scale=scale))\n",
    "\n",
    "    if dist_name == \"LP3\":\n",
    "        skew, loc, scale = params\n",
    "        yq = pearson3.ppf(p, skew, loc=loc, scale=scale)\n",
    "        return float(10**yq)\n",
    "\n",
    "    raise ValueError(f\"Unknown dist_name: {dist_name}\")\n",
    "\n",
    "def _loglik(dist_name: str, x: np.ndarray, params: tuple):\n",
    "    x = _clean_sample(x)\n",
    "    if dist_name == \"Normal\":\n",
    "        mu, sigma = params\n",
    "        return np.sum(norm.logpdf(x, loc=mu, scale=sigma)), 2\n",
    "    if dist_name == \"Gumbel\":\n",
    "        loc, scale = params\n",
    "        return np.sum(gumbel_r.logpdf(x, loc=loc, scale=scale)), 2\n",
    "    if dist_name == \"GEV\":\n",
    "        c, loc, scale = params\n",
    "        return np.sum(gev.logpdf(x, c, loc=loc, scale=scale)), 3\n",
    "    if dist_name == \"LP3\":\n",
    "        x = x[x > 0]\n",
    "        y = np.log10(x)\n",
    "        skew, loc, scale = params\n",
    "        ll_y = np.sum(pearson3.logpdf(y, skew, loc=loc, scale=scale))\n",
    "        ll_jac = -np.sum(np.log(x * np.log(10.0)))\n",
    "        return ll_y + ll_jac, 3\n",
    "    raise ValueError(dist_name)\n",
    "\n",
    "def gof_summary(x, dist_name: str, params: tuple):\n",
    "    x = _clean_sample(x)\n",
    "    n = x.size\n",
    "\n",
    "    if dist_name == \"LP3\":\n",
    "        # KS in log space (common practice)\n",
    "        x_pos = x[x > 0]\n",
    "        y = np.log10(x_pos)\n",
    "        skew, loc, scale = params\n",
    "        D, p = kstest(y, \"pearson3\", args=(skew, loc, scale))\n",
    "    elif dist_name == \"Normal\":\n",
    "        mu, sigma = params\n",
    "        D, p = kstest(x, \"norm\", args=(mu, sigma))\n",
    "    elif dist_name == \"Gumbel\":\n",
    "        loc, scale = params\n",
    "        D, p = kstest(x, \"gumbel_r\", args=(loc, scale))\n",
    "    elif dist_name == \"GEV\":\n",
    "        c, loc, scale = params\n",
    "        D, p = kstest(x, \"genextreme\", args=(c, loc, scale))\n",
    "    else:\n",
    "        raise ValueError(dist_name)\n",
    "\n",
    "    ll, k = _loglik(dist_name, x, params)\n",
    "    aic = 2*k - 2*ll\n",
    "    bic = k*np.log(n) - 2*ll\n",
    "\n",
    "    return {\n",
    "        \"n\": int(n),\n",
    "        \"KS_D\": float(D),\n",
    "        \"KS_p\": float(p),\n",
    "        \"logLik\": float(ll),\n",
    "        \"AIC\": float(aic),\n",
    "        \"BIC\": float(bic),\n",
    "    }\n",
    "\n",
    "def fit_and_compare(x):\n",
    "    x = _clean_sample(x)\n",
    "\n",
    "    fits = {\n",
    "        \"Normal\": fit_normal(x),\n",
    "        \"Gumbel\": fit_gumbel(x),\n",
    "        \"GEV\": fit_gev(x),\n",
    "    }\n",
    "\n",
    "    # LP3 is optional\n",
    "    try:\n",
    "        fits[\"LP3\"] = fit_lp3(x)\n",
    "    except Exception:\n",
    "        fits[\"LP3\"] = None\n",
    "\n",
    "    rows = []\n",
    "    for name, params in fits.items():\n",
    "        if params is None:\n",
    "            rows.append({\"Distribution\": name, \"status\": \"skipped\"})\n",
    "            continue\n",
    "        s = gof_summary(x, name, params)\n",
    "        s[\"Distribution\"] = name\n",
    "        s[\"status\"] = \"ok\"\n",
    "        rows.append(s)\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"status\", \"AIC\"], ascending=[True, True])\n",
    "    return df, fits\n",
    "\n",
    "def return_level_table(fits: dict, return_periods):\n",
    "    rows = []\n",
    "    for dist_name, params in fits.items():\n",
    "        if params is None:\n",
    "            continue\n",
    "        row = {\"Distribution\": dist_name}\n",
    "        for T in return_periods:\n",
    "            row[f\"RL_T{int(T)}\"] = return_level(dist_name, params, T)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plots: PDF/CDF + QQ/PP\n",
    "# =========================\n",
    "\n",
    "def plot_diagnostics(x, fits, title=\"\", units=\"\"):\n",
    "    x = _clean_sample(x)\n",
    "    x = np.sort(x)\n",
    "    n = x.size\n",
    "    p_emp = (np.arange(1, n+1) - 0.44) / (n + 0.12)  # Blom plotting position\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "\n",
    "    # 1) Histogram + fitted PDFs\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(x, bins=15, density=True, alpha=0.35)\n",
    "    xx = np.linspace(x.min()*0.95, x.max()*1.05, 400)\n",
    "\n",
    "    for name, params in fits.items():\n",
    "        if params is None:\n",
    "            continue\n",
    "        if name == \"Normal\":\n",
    "            mu, sigma = params\n",
    "            ax.plot(xx, norm.pdf(xx, mu, sigma), label=name)\n",
    "        elif name == \"Gumbel\":\n",
    "            loc, scale = params\n",
    "            ax.plot(xx, gumbel_r.pdf(xx, loc=loc, scale=scale), label=name)\n",
    "        elif name == \"GEV\":\n",
    "            c, loc, scale = params\n",
    "            ax.plot(xx, gev.pdf(xx, c, loc=loc, scale=scale), label=name)\n",
    "        elif name == \"LP3\":\n",
    "            skew, loc, scale = params\n",
    "            xx_pos = xx[xx > 0]\n",
    "            y = np.log10(xx_pos)\n",
    "            pdf_y = pearson3.pdf(y, skew, loc=loc, scale=scale)\n",
    "            pdf_x = pdf_y / (xx_pos * np.log(10.0))\n",
    "            ax.plot(xx_pos, pdf_x, label=name)\n",
    "\n",
    "    ax.set_title(\"PDF / histogram\")\n",
    "    ax.set_xlabel(f\"Value {units}\".strip())\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\")\n",
    "\n",
    "    # 2) CDF comparison\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(x, p_emp, \"k.\", label=\"Empirical\")\n",
    "    for name, params in fits.items():\n",
    "        if params is None:\n",
    "            continue\n",
    "        if name == \"Normal\":\n",
    "            mu, sigma = params\n",
    "            ax.plot(x, norm.cdf(x, mu, sigma), label=name)\n",
    "        elif name == \"Gumbel\":\n",
    "            loc, scale = params\n",
    "            ax.plot(x, gumbel_r.cdf(x, loc=loc, scale=scale), label=name)\n",
    "        elif name == \"GEV\":\n",
    "            c, loc, scale = params\n",
    "            ax.plot(x, gev.cdf(x, c, loc=loc, scale=scale), label=name)\n",
    "        elif name == \"LP3\":\n",
    "            x_pos = x[x > 0]\n",
    "            skew, loc, scale = params\n",
    "            ax.plot(x_pos, pearson3.cdf(np.log10(x_pos), skew, loc=loc, scale=scale), label=name)\n",
    "    ax.set_title(\"CDF\")\n",
    "    ax.set_xlabel(f\"Value {units}\".strip())\n",
    "    ax.set_ylabel(\"Non-exceedance probability\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\")\n",
    "\n",
    "    # 3) QQ plot\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(x, x, \"k--\", lw=1)\n",
    "    for name, params in fits.items():\n",
    "        if params is None:\n",
    "            continue\n",
    "        if name == \"Normal\":\n",
    "            mu, sigma = params\n",
    "            q = norm.ppf(p_emp, mu, sigma)\n",
    "            ax.plot(q, x, \".\", label=name)\n",
    "        elif name == \"Gumbel\":\n",
    "            loc, scale = params\n",
    "            q = gumbel_r.ppf(p_emp, loc=loc, scale=scale)\n",
    "            ax.plot(q, x, \".\", label=name)\n",
    "        elif name == \"GEV\":\n",
    "            c, loc, scale = params\n",
    "            q = gev.ppf(p_emp, c, loc=loc, scale=scale)\n",
    "            ax.plot(q, x, \".\", label=name)\n",
    "        elif name == \"LP3\":\n",
    "            x_pos = x[x > 0]\n",
    "            p_pos = p_emp[-x_pos.size:]\n",
    "            skew, loc, scale = params\n",
    "            qy = pearson3.ppf(p_pos, skew, loc=loc, scale=scale)\n",
    "            ax.plot(10**qy, x_pos, \".\", label=name)\n",
    "    ax.set_title(\"Q-Q\")\n",
    "    ax.set_xlabel(\"Theoretical quantiles\")\n",
    "    ax.set_ylabel(\"Empirical quantiles\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\")\n",
    "\n",
    "    # 4) PP plot\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot([0,1],[0,1],\"k--\", lw=1)\n",
    "    for name, params in fits.items():\n",
    "        if params is None:\n",
    "            continue\n",
    "        if name == \"Normal\":\n",
    "            mu, sigma = params\n",
    "            p_mod = norm.cdf(x, mu, sigma)\n",
    "            ax.plot(p_mod, p_emp, \".\", label=name)\n",
    "        elif name == \"Gumbel\":\n",
    "            loc, scale = params\n",
    "            p_mod = gumbel_r.cdf(x, loc=loc, scale=scale)\n",
    "            ax.plot(p_mod, p_emp, \".\", label=name)\n",
    "        elif name == \"GEV\":\n",
    "            c, loc, scale = params\n",
    "            p_mod = gev.cdf(x, c, loc=loc, scale=scale)\n",
    "            ax.plot(p_mod, p_emp, \".\", label=name)\n",
    "        elif name == \"LP3\":\n",
    "            x_pos = x[x > 0]\n",
    "            p_pos = p_emp[-x_pos.size:]\n",
    "            skew, loc, scale = params\n",
    "            p_mod = pearson3.cdf(np.log10(x_pos), skew, loc=loc, scale=scale)\n",
    "            ax.plot(p_mod, p_pos, \".\", label=name)\n",
    "\n",
    "    ax.set_title(\"P-P\")\n",
    "    ax.set_xlabel(\"Model CDF\")\n",
    "    ax.set_ylabel(\"Empirical CDF\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=4, frameon=False)\n",
    "    fig.tight_layout(rect=[0, 0.06, 1, 0.95])\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c3564",
   "metadata": {},
   "source": [
    "## Example A — 1D frequency analysis (region-mean annual series)\n",
    "\n",
    "This example:\n",
    "1. Opens NetCDF runoff data (monthly)\n",
    "2. Clips to your region polygon (shapefile)\n",
    "3. Aggregates to annual series (total or max)\n",
    "4. Creates a **region-mean** 1D sample (one value per year)\n",
    "5. Fits distributions, prints GOF table, and computes return levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load, clip, and build annual region-mean series\n",
    "# =========================\n",
    "\n",
    "ds_hist = _open_netcdf(RUNOFF_HIST_FILE)\n",
    "var = _find_data_var(ds_hist, VAR_HINT)\n",
    "da_hist = ds_hist[var]\n",
    "\n",
    "if RUNOFF_FUT_FILE:\n",
    "    ds_fut = _open_netcdf(RUNOFF_FUT_FILE)\n",
    "    var2 = _find_data_var(ds_fut, VAR_HINT)\n",
    "    da_fut = ds_fut[var2]\n",
    "    da = xr.concat([da_hist, da_fut], dim=\"time\")\n",
    "else:\n",
    "    da = da_hist\n",
    "\n",
    "# Clip to region (optional but typical for state/basin analysis)\n",
    "da_reg = clip_to_region(da, REGION_SHP, id_field=REGION_ID_FIELD, id_value=None)\n",
    "\n",
    "# Annual aggregation\n",
    "ann = annual_aggregate(da_reg, statistic=ANNUAL_STATISTIC, water_year=USE_WATER_YEAR)\n",
    "\n",
    "# Slice to a period and compute region mean time series\n",
    "ann_hist = ann.sel(year=slice(HIST_Y0, HIST_Y1))\n",
    "spatial_dims = [d for d in ann_hist.dims if d != \"year\"]\n",
    "x_hist = ann_hist.mean(dim=spatial_dims, skipna=True).values\n",
    "\n",
    "print(f\"Sample size (historical): {np.isfinite(x_hist).sum()} years\")\n",
    "\n",
    "# Fit & compare\n",
    "gof_df, fits = fit_and_compare(x_hist)\n",
    "display(gof_df)\n",
    "\n",
    "# Return levels\n",
    "rl_df = return_level_table(fits, RETURN_PERIODS)\n",
    "display(rl_df)\n",
    "\n",
    "# Diagnostics plots\n",
    "fig = plot_diagnostics(\n",
    "    x_hist,\n",
    "    fits,\n",
    "    title=f\"Region-mean annual {ANNUAL_STATISTIC} runoff ({HIST_Y0}–{HIST_Y1})\",\n",
    "    units=\"(native units)\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf571f4",
   "metadata": {},
   "source": [
    "## Example B — Pixel-wise return level map (optional)\n",
    "\n",
    "This section demonstrates how to compute a return level map using `xarray.apply_ufunc` (parallel-friendly with Dask).\n",
    "\n",
    "Warning: Pixel-wise fits for GEV/Gumbel/LP3 can be computationally expensive for large grids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f605307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Pixel-wise RL map (choose distribution)\n",
    "# =========================\n",
    "\n",
    "DIST_FOR_MAP = \"GEV\"     # \"Normal\", \"Gumbel\", \"GEV\", \"LP3\"\n",
    "T_MAP = 100\n",
    "\n",
    "ann_map = ann.sel(year=slice(HIST_Y0, HIST_Y1))\n",
    "\n",
    "# Chunk for Dask (adjust to your machine)\n",
    "chunks = {d: -1 if d == \"year\" else min(200, ann_map.sizes[d]) for d in ann_map.dims}\n",
    "ann_map = ann_map.chunk(chunks)\n",
    "\n",
    "p = 1.0 - 1.0 / T_MAP\n",
    "\n",
    "def _rl_pixel(x):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size < 8:\n",
    "        return np.nan\n",
    "    try:\n",
    "        if DIST_FOR_MAP == \"Normal\":\n",
    "            mu, sigma = norm.fit(x)\n",
    "            return float(norm.ppf(p, loc=mu, scale=sigma))\n",
    "        if DIST_FOR_MAP == \"Gumbel\":\n",
    "            loc, scale = gumbel_r.fit(x)\n",
    "            return float(gumbel_r.ppf(p, loc=loc, scale=scale))\n",
    "        if DIST_FOR_MAP == \"GEV\":\n",
    "            c, loc, scale = gev.fit(x)\n",
    "            return float(gev.ppf(p, c, loc=loc, scale=scale))\n",
    "        if DIST_FOR_MAP == \"LP3\":\n",
    "            x = x[x > 0]\n",
    "            if x.size < 10:\n",
    "                return np.nan\n",
    "            y = np.log10(x)\n",
    "            skew, loc, scale = pearson3.fit(y)\n",
    "            yq = pearson3.ppf(p, skew, loc=loc, scale=scale)\n",
    "            return float(10**yq)\n",
    "        raise ValueError(DIST_FOR_MAP)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "rl_map = xr.apply_ufunc(\n",
    "    _rl_pixel, ann_map,\n",
    "    input_core_dims=[[\"year\"]],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    ").rename(f\"RL{T_MAP}_{DIST_FOR_MAP}\")\n",
    "\n",
    "rl_map.attrs.update({\n",
    "    \"long_name\": f\"{T_MAP}-year return level from annual {ANNUAL_STATISTIC} ({DIST_FOR_MAP})\",\n",
    "    \"period\": f\"{HIST_Y0}-{HIST_Y1}\",\n",
    "    \"units\": \"native\",\n",
    "    \"p_non_exceed\": float(p),\n",
    "})\n",
    "\n",
    "# Compute and export\n",
    "out_nc = os.path.join(OUT_DIR, f\"RL{T_MAP}_{DIST_FOR_MAP}_{HIST_Y0}_{HIST_Y1}.nc\")\n",
    "rl_map.compute().to_netcdf(out_nc)\n",
    "print(\"Saved:\", out_nc)\n",
    "\n",
    "# Quick plot (works best if in lat/lon)\n",
    "plt.figure(figsize=(8,5))\n",
    "rl_map.compute().plot()\n",
    "plt.title(f\"RL{T_MAP} ({DIST_FOR_MAP}) — {HIST_Y0}–{HIST_Y1}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
